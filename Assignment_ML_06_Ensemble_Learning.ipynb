{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "   - Ensemble learning in machine learning is a technique where we combine several different models to make better predictions than a single model alone. The key idea behind it is that a group of weak or average models, when put together, can perform stronger and more accurately. Each model in the group, called a “base learner,” may make different kinds of mistakes, but when their outputs are combined for example, by voting or averaging, these errors can cancel out. This helps improve the overall performance, accuracy, and stability of the system. Common ensemble methods include Bagging, Boosting, and Stacking.\n",
        "\n",
        "2. What is the difference between Bagging and Boosting?\n",
        "   - Bagging and Boosting are both ensemble learning methods, but they work in different ways. Bagging builds multiple models independently using different random samples of the training data, and then combines their results by voting or averaging. This method helps reduce variance and prevents overfitting, making the model more stable. An example of bagging is the Random Forest algorithm. Boosting, on the other hand, builds models one after another, where each new model focuses more on the errors made by the previous ones. It combines the models by giving more weight to the stronger ones, which helps reduce bias and improve accuracy. However, Boosting can sometimes overfit if the data is noisy. Examples of boosting algorithms include AdaBoost and Gradient Boosting.\n",
        "\n",
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "   - Bootstrap sampling is a technique in which new training datasets are created by randomly selecting data points with replacement from the original dataset. This means some samples may appear more than once, while others may not appear at all. In Bagging methods like Random Forest, bootstrap sampling is very important because it ensures that each decision tree is trained on a slightly different version of the data. As a result, the trees become diverse and make different errors. When their predictions are combined, usually by voting or averaging, the final result becomes more accurate and stable. This helps reduce overfitting and improves the overall performance of the model.\n",
        "\n",
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "   - Out-of-Bag (OOB) samples are the data points that are not included in a particular bootstrap sample when training models in ensemble methods like Random Forest. Since each model is trained on a random subset of the data, around one-third of the original data is usually left out in each round, and these unused samples are called OOB samples. The OOB score is then used to evaluate the model’s performance without needing a separate test set. After training, each OOB sample is predicted only by the models that did not use it during training, and the accuracy of these predictions is calculated. This gives a reliable estimate of how well the model performs on unseen data.\n",
        "\n",
        "\n",
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "    - In a single Decision Tree, feature importance is calculated based on how much each feature helps reduce impurity, such as Gini impurity or entropy, when splitting the data. The more a feature contributes to making accurate splits, the higher its importance. However, the results from a single tree can be unstable because the tree is built on one dataset and may overfit to it. In a Random Forest, feature importance is determined by averaging the importance scores of each feature across many decision trees, each trained on different bootstrap samples and random subsets of features. This makes the results more reliable, balanced, and less affected by noise. Therefore, feature importance in a Random Forest is generally more accurate and stable than in a single Decision Tree.\n",
        "\n"
      ],
      "metadata": {
        "id": "mQZxuetuI8sV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYJr7EXnI7Ad",
        "outputId": "f6a7e8aa-fa77-40db-cc4f-9be47f208398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most important features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ],
      "source": [
        "# 6.  Write a Python program to:\n",
        "# ● Load the Breast Cancer dataset using\n",
        "# sklearn.datasets.load_breast_cancer()\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort features by importance and get top 5\n",
        "top_features = feature_importance_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "\n",
        "# Print the top 5 features\n",
        "print(\"Top 5 most important features:\")\n",
        "print(top_features)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  7. Write a Python program to:\n",
        "# ● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "# ● Evaluate its accuracy and compare with a single Decision Tree\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"Accuracy of single Decision Tree: {round(accuracy_dt,4)}\")\n",
        "\n",
        "# Train a Bagging Classifier using Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,  # Number of trees\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "accuracy_bag = accuracy_score(y_test, y_pred_bag)\n",
        "print(f\"Accuracy of Bagging Classifier: {round(accuracy_bag,4)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFSXmnbPLg-d",
        "outputId": "6b016ab5-f062-4062-b590-2b9c181b6b47"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to:\n",
        "# ● Train a Random Forest Classifier\n",
        "# ● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "# ● Print the best parameters and final accuracy\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Random Forest classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 2, 4, 6, 8]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV to find the best hyperparameters\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the final model on the test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Final accuracy on test set: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13YG4WLuL6ku",
        "outputId": "8d6a97b4-f780-4f8a-a677-72467abb35cc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Final accuracy on test set: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  9. Write a Python program to:\n",
        "# ● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "# Housing dataset\n",
        "# ● Compare their Mean Squared Errors (MSE)\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Bagging Regressor using Decision Trees\n",
        "bagging = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bagging = bagging.predict(X_test)\n",
        "mse_bagging = mean_squared_error(y_test, y_pred_bagging)\n",
        "print(f\"Mean Squared Error of Bagging Regressor: {round(mse_bagging,4)}\")\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "print(f\"Mean Squared Error of Random Forest Regressor: {round(mse_rf,4)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpHo1oltMef-",
        "outputId": "74fd756d-a01d-41bb-d3a0-0ad4da834ebb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error of Bagging Regressor: 0.2579\n",
            "Mean Squared Error of Random Forest Regressor: 0.2577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data scientist at a financial institution to predict loandefault. You have access to customer demographic and transaction history data.You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "    - To predict loan defaults using ensemble techniques, I would start by choosing between Bagging and Boosting based on the dataset and model behavior. Bagging, like Random Forest, is useful if individual models, such as Decision Trees, tend to overfit, because it reduces variance by averaging predictions over multiple models. Boosting, like XGBoost or AdaBoost, is better if models underfit, as it sequentially focuses on correcting errors to reduce bias. To handle overfitting, I would control model complexity through parameters like tree depth, use regularization and apply cross validation to ensure generalization. Base models would be selected to be diverse and complementary Decision Trees are common for Bagging due to their high variance, while shallow trees or weak learners are ideal for Boosting. Performance would be evaluated using k fold cross validation and metrics like accuracy, precision, recall, F1-score, and AUC-ROC to ensure reliability. Ensemble learning improves decision making in this real world context by combining multiple models to produce more stable and accurate predictions, which helps the financial institution identify high risk customers, manage risk effectively, and make fair lending decisions while minimizing potential losses.\n"
      ],
      "metadata": {
        "id": "xK6s_1OYNnO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, roc_auc_score\n",
        "\n",
        "# Simulate a loan default dataset\n",
        "X, y = make_classification(n_samples=5000, n_features=20, n_informative=15,\n",
        "                           n_redundant=5, n_classes=2, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ----------------- Bagging Classifier -----------------\n",
        "bagging = BaggingClassifier(\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "accuracy_bag = accuracy_score(y_test, y_pred_bag)\n",
        "auc_bag = roc_auc_score(y_test, bagging.predict_proba(X_test)[:,1])\n",
        "\n",
        "# ----------------- Boosting Classifier -----------------\n",
        "boosting = GradientBoostingClassifier(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "boosting.fit(X_train, y_train)\n",
        "y_pred_boost = boosting.predict(X_test)\n",
        "accuracy_boost = accuracy_score(y_test, y_pred_boost)\n",
        "auc_boost = roc_auc_score(y_test, boosting.predict_proba(X_test)[:,1])\n",
        "\n",
        "# ----------------- Cross-Validation -----------------\n",
        "cv_scores_bag = cross_val_score(bagging, X_train, y_train, cv=5, scoring='accuracy')\n",
        "cv_scores_boost = cross_val_score(boosting, X_train, y_train, cv=5, scoring='accuracy')\n",
        "\n",
        "# ----------------- Print Results -----------------\n",
        "print(\"Bagging Classifier Accuracy:\", accuracy_bag)\n",
        "print(\"Bagging Classifier AUC-ROC:\", auc_bag)\n",
        "print(\"Bagging CV Accuracy:\", np.mean(cv_scores_bag))\n",
        "\n",
        "print(\"\\nBoosting Classifier Accuracy:\", accuracy_boost)\n",
        "print(\"Boosting Classifier AUC-ROC:\", auc_boost)\n",
        "print(\"Boosting CV Accuracy:\", np.mean(cv_scores_boost))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "of4uRoNnNIUc",
        "outputId": "dcc59f1c-bb61-4b1e-c1f0-c36984548c93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Classifier Accuracy: 0.9093333333333333\n",
            "Bagging Classifier AUC-ROC: 0.9671308523409363\n",
            "Bagging CV Accuracy: 0.916\n",
            "\n",
            "Boosting Classifier Accuracy: 0.908\n",
            "Boosting Classifier AUC-ROC: 0.9670997287804011\n",
            "Boosting CV Accuracy: 0.9151428571428571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sSlYYEhYOVMd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}