{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree, and how does it work in the context of classification?\n",
        "   - A Decision Tree is a type of machine learning model used to make decisions or predictions based on data. In classification, it works by splitting the data into smaller groups based on certain rules or conditions related to the features of the data. The tree starts from a root node and branches out into different paths based on the answers or outcomes of that feature. Each internal node represents a decision based on a feature, and each leaf node represents a final class or outcome. The goal of the Decision Tree is to divide the data in a way that separates the different classes as clearly as possible, helping to make accurate predictions for new data.\n",
        "\n",
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "   - Gini Impurity and Entropy are measures used in Decision Trees to check how mixed or impure the data is before making a split. Gini Impurity shows how often a randomly chosen item would be incorrectly classified if it were labeled according to the class distribution in the group, while Entropy measures the amount of uncertainty or disorder in the data. Both aim to find how pure each node is lower values mean the data mostly belongs to one class. When building a Decision Tree, the algorithm uses these measures to decide the best places to split the data so that each new group becomes as pure as possible. In short, Gini Impurity and Entropy help the tree choose the most effective splits to improve classification accuracy.\n",
        "\n",
        "\n",
        "3. What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "   - Pre-pruning and post-pruning are techniques used to prevent a Decision Tree from becoming too complex and overfitting the data. Pre-pruning stops the tree from growing once it reaches a certain condition, such as a maximum depth or a minimum number of samples in a node. This means the tree is simplified during its construction. A practical advantage of pre-pruning is that it saves time and computational resources since the tree stops growing early. Post-pruning, on the other hand, allows the tree to grow fully and then removes or trims branches that do not improve accuracy on a validation dataset. A practical advantage of post-pruning is that it often results in better model performance because it considers the full tree before deciding which parts to remove.\n",
        "\n",
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "   - Information Gain is a measure used in Decision Trees to decide which feature to split on at each step. It tells us how much 'information' or certainty we gain about the target class after splitting the data based on a particular feature. In simple terms, it compares the impurity of the parent node with the impurity of the child nodes created after the split. A higher Information Gain means that the split makes the data more organized and helps the tree classify examples more accurately. It is important because it guides the tree to choose the feature that best separates the classes, leading to more accurate and efficient decision making.\n",
        "\n",
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "   - Decision Trees are widely used in many real world applications because they are easy to understand and interpret. Some common uses include medical diagnosis, where they help doctors decide the likelihood of a disease, finance, for assessing credit risk or detecting fraud ,marketing, for predicting customer behavior and manufacturing, for quality control and process optimization. The main advantages of Decision Trees are that they are simple to visualize, handle both numerical and categorical data, and don’t require much data preparation. However, their main limitations are that they can easily overfit the data, especially if the tree is too deep, and they can be sensitive to small changes in the data, which might lead to very different trees. Despite these drawbacks, Decision Trees remain popular due to their clarity and effectiveness in many practical problems.\n"
      ],
      "metadata": {
        "id": "q9wYRGfxZbWr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YdliP5zZKzm",
        "outputId": "d47d3d22-34f9-40c2-f68c-abdd0a517dca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0\n",
            "sepal width (cm): 0.02\n",
            "petal length (cm): 0.89\n",
            "petal width (cm): 0.09\n"
          ]
        }
      ],
      "source": [
        "# Question 6: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier using the Gini criterion\n",
        "# ● Print the model’s accuracy and feature importances\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = load_iris()\n",
        "X = df.data\n",
        "y = df.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for name, importance in zip(df.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {round(importance,2)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "# a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "df = load_iris()\n",
        "X = df.data\n",
        "y = df.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Classifier with max_depth=3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "\n",
        "# Train a fully-grown Decision Tree Classifier\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "\n",
        "# Calculate accuracies\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(\"Decision Tree (max_depth=3) Accuracy:\", accuracy_limited)\n",
        "print(\"Fully-Grown Decision Tree Accuracy:\", accuracy_full)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfmRd44LcZOz",
        "outputId": "f8596f96-6d32-4a11-b4bb-a39b417f803d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree (max_depth=3) Accuracy: 1.0\n",
            "Fully-Grown Decision Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to:\n",
        "# ● Load the Boston Housing Dataset\n",
        "# ● Train a Decision Tree Regressor\n",
        "# ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the dataset\n",
        "df = fetch_california_housing()\n",
        "X = df.data\n",
        "y = df.target\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(df.feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {round(importance,2)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx2QrdH3opR0",
        "outputId": "7446b318-39a2-4e62-abbf-8a4a5df2bbd6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.5280096503174904\n",
            "\n",
            "Feature Importances:\n",
            "MedInc: 0.52\n",
            "HouseAge: 0.05\n",
            "AveRooms: 0.05\n",
            "AveBedrms: 0.02\n",
            "Population: 0.03\n",
            "AveOccup: 0.14\n",
            "Latitude: 0.09\n",
            "Longitude: 0.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to:\n",
        "# ● Load the Iris Dataset\n",
        "# ● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "# GridSearchCV\n",
        "# ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "df = load_iris()\n",
        "X = df.data\n",
        "y = df.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define the Decision Tree classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10, 15]\n",
        "}\n",
        "\n",
        "# Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLGmpl65pYO0",
        "outputId": "637780e3-33cc-48e7-f200-b02b550fe712"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "    - To build a Decision Tree model for predicting whether a patient has a disease, the first step is to handle missing values by identifying them and imputing appropriately numerical features can be filled with the mean or median, while categorical features can use the mode or a new “Unknown” category. Next, categorical features must be encoded into numeric form, using one hot encoding for nominal variables and ordinal encoding for features with a natural order. After preprocessing, the dataset is split into training and testing sets, and a Decision Tree Classifier is trained on the training data, allowing us to also examine feature importances. To improve performance and prevent overfitting, hyperparameters such as max_depth, min_samples_split, and min_samples_leaf are tuned using GridSearchCV or RandomizedSearchCV with cross validation. The model is then evaluated on the test set using metrics like accuracy, precision, recall, F1-score, and ROC-AUC, which are especially important if the disease is rare. In a real world healthcare setting, this model provides significant business value by identifying high risk patients early, guiding preventive care, reducing healthcare costs, and offering interpretable insights into which patient factors contribute most to disease risk, ultimately supporting better clinical decisions and resource allocation."
      ],
      "metadata": {
        "id": "cJhE6wd3qjcC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5949ultbqwSC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}